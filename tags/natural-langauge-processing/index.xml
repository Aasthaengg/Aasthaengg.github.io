<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural Langauge Processing on aasthasingh</title>
    <link>https://aasthaengg.github.io/tags/natural-langauge-processing/</link>
    <description>Recent content in Natural Langauge Processing on aasthasingh</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 05 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://aasthaengg.github.io/tags/natural-langauge-processing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Speed up the development with advanced pair programming: GitHub Copilot</title>
      <link>https://aasthaengg.github.io/posts/github-copilot/</link>
      <pubDate>Mon, 05 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://aasthaengg.github.io/posts/github-copilot/</guid>
      <description>Speed up the development with advanced pair programming: GitHub Copilot     Something Microsoft couldn’t get their hands off of, and you probably won’t be able to either!
 Hello technophiles, Welcome to my blog! Read along to know political, technical, and public understandings and viewpoints on the great magical AI algorithm launched this week.
 “Anyone who puts in money at OpenAI can only expect returns 100 times their investment”</description>
    </item>
    
    <item>
      <title>Evolving with BERT: Introduction to RoBERTa</title>
      <link>https://aasthaengg.github.io/posts/roberta/</link>
      <pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://aasthaengg.github.io/posts/roberta/</guid>
      <description>Evolving with BERT: Introduction to RoBERTa     No matter how good it is, it can always get better, and that’s the exciting part.
 In this article, I will discuss the &amp;ldquo;exciting part,&amp;rdquo; which was how the Facebook Research AI agency modified the training procedure of the existing Google BERT, proving to the world that there is always room to improve.
Let’s look at the development of a robustly optimized method for pretraining natural language processing (NLP) systems(RoBERTa).</description>
    </item>
    
    <item>
      <title>Auto-code generation using GPT-2</title>
      <link>https://aasthaengg.github.io/posts/code-gpt2/</link>
      <pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://aasthaengg.github.io/posts/code-gpt2/</guid>
      <description>Auto-code generation using GPT-2    About GPT-2     GPT-2 stands for “Generative Predictive Transformer”. It is an open-source model trained on an over 1.5 Billion parameters for generating the next sequence of text, for a give sequence.
  The GPT-2 has a remarkable and amazing competence to generate texts, much beyond the expectations of conventional language models.
 “Too dangerous to be released.”    A phrase published the press statement by OpenAI to accompany their announcement of the release of their GPT-2 language model in February 2019.</description>
    </item>
    
    <item>
      <title>Key Feature extraction from classified summary of a Text file using BERT</title>
      <link>https://aasthaengg.github.io/posts/text-summary/</link>
      <pubDate>Mon, 31 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://aasthaengg.github.io/posts/text-summary/</guid>
      <description>Harnessing the power of BERT embeddings    In this post, I’ll show you how BERT solves a basic text summarization and categorization issue.
About BERT (Bidirectional Encoder Representations from Transformers)     BERT, in a nutshell, is a model that understands how to represent text. You feed it a sequence, and it scans left and right a number of times before producing a vector representation for each word as an output.</description>
    </item>
    
  </channel>
</rss>
