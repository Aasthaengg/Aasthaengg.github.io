<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">
    <meta name="color-scheme" content="light dark">

    
      <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests; block-all-mixed-content; default-src 'self'; child-src 'self'; font-src 'self' https://fonts.gstatic.com https://cdn.jsdelivr.net/; form-action 'self'; frame-src 'self'; img-src 'self'; object-src 'none'; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com/ https://cdn.jsdelivr.net/; script-src 'self' 'unsafe-inline' https://www.google-analytics.com; prefetch-src 'self'; connect-src 'self' https://www.google-analytics.com;">

    

    <meta name="author" content="Aastha Singh">
    <meta name="description" content="Evolving with BERT: Introduction to RoBERTa     No matter how good it is, it can always get better, and that’s the exciting part.
 In this article, I will discuss the &ldquo;exciting part,&rdquo; which was how the Facebook Research AI agency modified the training procedure of the existing Google BERT, proving to the world that there is always room to improve.
Let’s look at the development of a robustly optimized method for pretraining natural language processing (NLP) systems(RoBERTa).">
    <meta name="keywords" content="blog,developer,personal">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Evolving with BERT: Introduction to RoBERTa"/>
<meta name="twitter:description" content="Evolving with BERT: Introduction to RoBERTa     No matter how good it is, it can always get better, and that’s the exciting part.
 In this article, I will discuss the &ldquo;exciting part,&rdquo; which was how the Facebook Research AI agency modified the training procedure of the existing Google BERT, proving to the world that there is always room to improve.
Let’s look at the development of a robustly optimized method for pretraining natural language processing (NLP) systems(RoBERTa)."/>

    <meta property="og:title" content="Evolving with BERT: Introduction to RoBERTa" />
<meta property="og:description" content="Evolving with BERT: Introduction to RoBERTa     No matter how good it is, it can always get better, and that’s the exciting part.
 In this article, I will discuss the &ldquo;exciting part,&rdquo; which was how the Facebook Research AI agency modified the training procedure of the existing Google BERT, proving to the world that there is always room to improve.
Let’s look at the development of a robustly optimized method for pretraining natural language processing (NLP) systems(RoBERTa)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aasthaengg.github.io/posts/roberta/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-06-28T00:00:00+00:00" />
<meta property="article:modified_time" content="2021-06-28T00:00:00+00:00" />



    <title>
  Evolving with BERT: Introduction to RoBERTa · aasthasingh
</title>

    
      <link rel="canonical" href="https://aasthaengg.github.io/posts/roberta/">
    

    <link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>

    
      
      
      <link rel="stylesheet" href="/css/coder.min.fd5282c052ba60fd838d6ee7ed1db78ad94f1e62938c66b9fbccff89ab345cc0.css" integrity="sha256-/VKCwFK6YP2DjW7n7R23itlPHmKTjGa5&#43;8z/ias0XMA=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="/css/coder-dark.min.ccbbada2e264e4fdbf9b2181cccc2cdb289a63dc9520a1e96ac2b9a45778df29.css" integrity="sha256-zLutouJk5P2/myGBzMws2yiaY9yVIKHpasK5pFd43yk=" crossorigin="anonymous" media="screen" />
      
    

    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    <meta name="generator" content="Hugo 0.89.4" />
  </head>

  
  
    
  
  <body class="preload-transitions colorscheme-dark">
    
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      aasthasingh
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">About Me</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/cv/">Resume</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/projects/">Projects</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/tags/">Tags</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">Contact me</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://aasthaengg.github.io/posts/roberta/">
              Evolving with BERT: Introduction to RoBERTa
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2021-06-28T00:00:00Z'>
                June 28, 2021
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              8-minute read
            </span>
          </div>
          
          
          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/deep-learning/">Deep Learning</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/natural-langauge-processing/">Natural Langauge Processing</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/bert/">BERT</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/roberta/">RoBERTa</a>
    </span></div>

        </div>
      </header>

      <div>
        
        <h1 id="evolving-with-bert-introduction-to-roberta">
  Evolving with BERT: Introduction to RoBERTa
  <a class="heading-link" href="#evolving-with-bert-introduction-to-roberta">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<blockquote>
<p>No matter how good it is, it can always get better, and that’s the exciting part.</p>
</blockquote>
<p>In this article, I will discuss the &ldquo;exciting part,&rdquo; which was how the
<a href="https://medium.com/u/25aae929dbb1?source=post_page-----5174ec0e7c82-----------------------------------">Facebook Research</a> AI agency modified the training procedure of the existing <a href="https://medium.com/u/be36e94a7e47?source=post_page-----5174ec0e7c82-----------------------------------">Google</a> BERT, proving to the world that there is always room to improve.</p>
<p>Let’s look at the development of a robustly optimized method for pretraining natural language processing (NLP) systems(RoBERTa).</p>
<figure><img src="/images/google-fb.jpg"/>
</figure>

<h2 id="open-source-bert-by-google">
  Open Source BERT by Google
  <a class="heading-link" href="#open-source-bert-by-google">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>Bidirectional Encoder Representations from Transformers, or BERT, is a self-supervised method <strong>released by Google in 2018</strong>.</p>
<blockquote>
<p>BERT is a tool/model which understand language beter than any other model in the history. It’s freely available and it is increadibly versatily as it can solve a large number of problems related to lanugage tasks. You have used bert without even knowing!</p>
<p>If you have used google serch, you have already used BERT</p>
</blockquote>
<h2 id="architecture">
  Architecture
  <a class="heading-link" href="#architecture">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<h3 id="transformer-model--a-foundational-concept-for-bert">
  Transformer model — a foundational concept for BERT
  <a class="heading-link" href="#transformer-model--a-foundational-concept-for-bert">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>BERT is based on the Transformer model architecture
Examining the model as if it were a single black box, a machine translation application would take a sentence in one language and translate it into a different language.</p>
<figure><img src="/images/transformer-mt.png"/><figcaption>
            <h4>Transformer performing the task of machine translation</h4>
        </figcaption>
</figure>

<ul>
<li>Basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task.</li>
</ul>
<figure><img src="/images/transformer-structure.png"/><figcaption>
            <h4>The basic structure of a transformer</h4>
        </figcaption>
</figure>

<ul>
<li>Since BERT’s goal is to generate a language representation model, it only needs the encoder part. hence, BERT is basically a trained <strong>Transformer Encoder stack</strong></li>
</ul>
<figure><img src="/images/encoder-structure.png"/><figcaption>
            <h4>A basic structure of encoder block</h4>
        </figcaption>
</figure>

<p>To understand further implementations of its mechanism, refer to my previous blog.</p>
<h3 id="training-of-bert">
  Training of BERT
  <a class="heading-link" href="#training-of-bert">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>During pretraining, BERT uses two objectives: masked language modeling and next sentence prediction.</p>
<ul>
<li><strong>Masked Language Modeling</strong> (MLM) basically <strong>masks</strong> 80% of the 15% of the randomly selected input tokens and uses the other tokens to attempt to predict the mask (missing word).</li>
</ul>
<blockquote>
<p>Input: The Rose is [MASK1]. It smells [MASK2].</p>
<p>O/P Label: [MASK1] = Red; [MASK2] = Good</p>
</blockquote>
<ul>
<li><strong>Next Sentence Prediction</strong> (NSP)is a binary classification loss for predicting whether two segments follow each other or are from a different document to create a semantic meaning.</li>
</ul>
<blockquote>
<p>Sentence 1: The Rose is Red.
Sentence 2: It tastes good.
Sentence 3: It smells good.</p>
</blockquote>
<h2 id="beginning-of-the-optimization-of-bert-introduction-to-roberta">
  Beginning of the Optimization of BERT: Introduction to RoBERTa
  <a class="heading-link" href="#beginning-of-the-optimization-of-bert-introduction-to-roberta">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<h3 id="room-for-improvement-in-bert">
  Room for improvement in BERT
  <a class="heading-link" href="#room-for-improvement-in-bert">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<figure><img src="/images/bert-learn.gif"/><figcaption>
            <h4>Learning scope of improvements in BERT</h4>
        </figcaption>
</figure>

<p><strong>BERT is significantly undertrained and the following areas stand the scope of modifications.</strong></p>
<ol>
<li>
<p><strong>Masking in BERT training</strong>
The masking is done only once during data preprocessing, resulting in a single static mask. Hence, the same input masks were fed to the model on every single epoch.</p>
</li>
<li>
<p><strong>Next Sentence Prediction</strong></p>
</li>
</ol>
<ul>
<li>
<p>The original input format used in BERT is <strong>SEGMENT-PAIR+NSP LOSS</strong>.</p>
</li>
<li>
<p>In this, each input has a pair of segments, which can each contain multiple natural sentences, but the total combined length must be less than 512 tokens.</p>
</li>
<li>
<p>It is noticed that individual sentences hurt performance on downstream tasks, which according to the hypothesis is because the model was not able to learn long-range dependencies, hence the authors could experiment with removing/adding NSP loss to see the effect in the model’s performance.</p>
</li>
</ul>
<ol start="3">
<li>
<p><strong>Text Encoding</strong>
The original BERT implementation uses a character-level BPE vocabulary of size 30K.
BERT uses the WordPiece method a language-modeling-based variant of Byte Pair Encoding.</p>
</li>
<li>
<p><strong>Training batch size</strong>
Originally BERT is trained for <strong>1M steps with a batch size of 256 sequences</strong>, which shows room for improvement in perplexity on the Masked Language Modelling objective.</p>
</li>
</ol>
<h3 id="altering-the-training-procedure">
  Altering the training Procedure
  <a class="heading-link" href="#altering-the-training-procedure">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<ol>
<li><strong>Replacing Static masking with Dynamic Masking</strong></li>
</ol>
<p>To avoid masking the same word multiple times, Facebook used Dynamic masking; the training data was repeated 10 times and every next time, the masked word would be different, meaning the sentence would be the same but the words masked would be different.</p>
<ol start="2">
<li><strong>Removing NSP</strong></li>
</ol>
<p><strong>TEST 1: Feeding the following alternate training formats.</strong></p>
<ul>
<li>
<p>2.1 <strong>Retain NSP Loss</strong></p>
<p><strong>SENTENCE-PAIR+NSP</strong> - Each input contains a pair of natural sentences, sampled from a contiguous portion of one document or separate documents. The NSP loss is retained.</p>
</li>
<li>
<p>2.2 <strong>Remove NSP loss</strong></p>
<p><strong>FULL-SENTENCES</strong> Each input is packed with full sentences sampled contiguously from single or cross documents, such that the total length is at most 512 tokens. We remove the NSP loss.</p>
<p><strong>DOC-SENTENCES</strong> Inputs are constructed similarly to FULL-SENTENCES, except that they may not cross document boundaries. Inputs sampled near the end of a document may be shorter than 512 tokens, so we dynamically increase the batch size in these cases to achieve a similar number of total tokens as FULLSENTENCES. We remove the NSP loss.</p>
</li>
</ul>
<p><strong>Results</strong></p>
<ul>
<li>
<p>Removing the NSP loss matches or slightly improves downstream task performance, in contrast to the original BERT with NSP loss.</p>
</li>
<li>
<p>The sequences from a single document (DOC-SENTENCES) perform slightly better than packing sequences from multiple documents (FULL-SENTENCES) as shown in Table 1.</p>
</li>
</ul>
<figure><img src="/images/nlp-loss.png"/><figcaption>
            <h4>Table 1: Comparison of performance of models with and without NSP loss</h4>
        </figcaption>
</figure>

<ol start="3">
<li><strong>Training with large mini-batch</strong></li>
</ol>
<p>It is noticed that training a model with large mini-batches improves the perplexity of MLM objective and end-accuracy.</p>
<blockquote>
<p>1M steps, batch size of 256 has equivalent computational cost to 31K steps, batch size of 8K.</p>
</blockquote>
<p>Large batches are also easier to parallelize via distributed parallel training.</p>
<ol start="4">
<li><strong>Byte-Pair Encoding</strong></li>
</ol>
<ul>
<li>
<p>Here Byte-Pair Encoding is used over raw bytes instead of Unicode characters.</p>
</li>
<li>
<p>The BPE subword vocabulary is reduced to 50K (still bigger than BERT’s vocab size) units.</p>
</li>
</ul>
<figure><img src="/images/bpe.png"/><figcaption>
            <h4>A quick example of Byte Pair Encoding (BPE)</h4>
        </figcaption>
</figure>

<p>Despite degrading the performance of end-task performance in some cases, this method was used for encoding as it is a universal encoding scheme that doesn’t need any preprocessing and tokenization rules.</p>
<ol start="5">
<li><strong>Increasing Training Data</strong></li>
</ol>
<p>It was observed that training BERT on larger datasets, greatly improves its performance. Hence the training data was increased to 160GB of uncompressed text.</p>
<figure><img src="/images/bert-impress.jpeg"/><figcaption>
            <h4>Google’s BERT after seeing the modifications!</h4>
        </figcaption>
</figure>

<!-- raw HTML omitted -->
<h2 id="facebooks-roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems">
  Facebook’s RoBERTa: An optimized method for pretraining self-supervised NLP systems
  <a class="heading-link" href="#facebooks-roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<figure><img src="/images/fb-roberta.gif"/>
</figure>

<p>The issues discussed above were identified by Facebook AI Research (FAIR), and hence, they proposed an ‘optimized’ and ‘robust’ version of BERT.</p>
<blockquote>
<p>RoBERTa is actually robust across NLU tasks, the absolute geniuses at Facebook actually did it, and it’s not a clickbait!</p>
</blockquote>
<blockquote>
<p><em>RoBERTa is part of Facebook’s ongoing commitment to advancing the state-of-the-art in self-supervised systems that can be developed with less reliance on time- and resource-intensive data labeling.</em></p>
</blockquote>
<p>It acts more like just giving context to the rest of the input (the actual context), which is a reason why RoBERTa works better than models that were actually fine-tuned on tasks like SQUAD.</p>
<h2 id="why-roberta-matters">
  Why RoBERTa matters?
  <a class="heading-link" href="#why-roberta-matters">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>The world appreciates Google as they have made this natural language processing program open source to the world. <strong>Through RoBERTA, we see this move to open source BERT has brought a drastic change to NLP</strong>.</p>
<p>The study demonstrates how sufficiently pre-trained models lead to improvements with trivial tasks when scaled to extremities.</p>
<blockquote>
<p><em><strong>Improving the potential of BERT, is hugely impacting the the market and economic outlook.</strong></em></p>
</blockquote>
<ul>
<li>
<p>BERT and RoBERTa are used in the improvement in NLP tasks as they make use of embedding vector space that is rich in context.</p>
</li>
<li>
<p>Using RoBERTa for preprocessing the data, is of major advancement to all the small product to big multinational companies as mainly work to incorporate data for analysis to extract information.</p>
</li>
</ul>
<p>This is significant because, as a result of these studies, experiments, and developments, we are getting closer to the larger challenge for NLP models, which is to achieve a human-level understanding of language!</p>
<h2 id="how-roberta-is-different-from-bert">
  How RoBERTa is different from BERT
  <a class="heading-link" href="#how-roberta-is-different-from-bert">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p><strong>The authors of RoBERTa suggest that BERT is largely undertrained and hence, they put forth the following improvements for the same</strong>.</p>
<ul>
<li>
<p>More <strong>training data</strong> (16G vs 160G)</p>
</li>
<li>
<p>Uses <strong>dynamic masking pattern</strong> instead of <strong>static masking pattern</strong>.</p>
</li>
<li>
<p>Replacing the <strong>next sentence prediction</strong> objective with <strong>full sentences without NSP</strong>.</p>
</li>
<li>
<p>Training on <strong>Longer Sequences</strong></p>
</li>
</ul>
<figure><img src="/images/roberta-accuracy.png"/><figcaption>
            <h4>Accuracy vs Number of training steps plot for different models.</h4>
        </figcaption>
</figure>

<figure><img src="/images/roberta-comparison.png"/><figcaption>
            <h4>Comparison of BERT, RoBERTa, DistilBERT, XLNet in terms of training strategies</h4>
        </figcaption>
</figure>

<h2 id="zero-shot-learning-with-roberta">
  Zero-Shot Learning with RoBERTa
  <a class="heading-link" href="#zero-shot-learning-with-roberta">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<h3 id="zero-shot-learning">
  Zero-Shot Learning
  <a class="heading-link" href="#zero-shot-learning">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>It is a type of Machine Learning technique, where the model is used without fine-tuning on a particular task.</p>
<figure><img src="/images/zero.png"/><figcaption>
            <h4>Explaining Zero-Shot Learning</h4>
        </figcaption>
</figure>

<p><strong>You can use the following examples to implement any text sequence classification task (One-Shot Classification) by simply following the steps. It is extensively used also for sequence regression tasks.</strong></p>
<h3 id="load-roberta-from-torchhub">
  Load RoBERTa from torch.hub
  <a class="heading-link" href="#load-roberta-from-torchhub">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>Loading pre-trained RoBERTa Large model using Pytorch</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">roberta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;pytorch/fairseq&#39;</span><span class="p">,</span> <span class="s1">&#39;roberta.large&#39;</span><span class="p">)</span>
<span class="n">roberta</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><h3 id="roberta-for-sequence-classification">
  Roberta For Sequence Classification
  <a class="heading-link" href="#roberta-for-sequence-classification">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>RoBERTa Model transformer is with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.</p>
<ol>
<li><strong>Download RoBERTa that is already finetuned for MNLI</strong></li>
</ol>
<p>RoBERTa is also pretrained on MNLI (Multi-Genre Natural Language Inference is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information).</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">roberta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;pytorch/fairseq&#39;</span><span class="p">,</span> <span class="s1">&#39;roberta.large.mnli&#39;</span><span class="p">)</span>
<span class="n">roberta</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># disable dropout for evaluation</span>
</code></pre></td></tr></table>
</div>
</div><ol start="2">
<li><strong>Encode a pair of sentences and make a prediction</strong></li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">tokens</span> <span class="o">=</span> <span class="n">roberta</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;Roberta is a heavily optimized version of BERT.&#39;</span><span class="p">,</span> <span class="s1">&#39;Roberta is not very optimized.&#39;</span><span class="p">)</span>
<span class="n">roberta</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s1">&#39;mnli&#39;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><ol start="3">
<li><strong>Encode another pair of sentences</strong></li>
</ol>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="n">tokens</span> <span class="o">=</span> <span class="n">roberta</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="err">‘</span><span class="n">Roberta</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">heavily</span> <span class="n">optimized</span> <span class="n">version</span> <span class="n">of</span> <span class="n">BERT</span><span class="o">.</span><span class="err">’</span><span class="p">,</span> <span class="err">‘</span><span class="n">Roberta</span> <span class="ow">is</span> <span class="n">based</span> <span class="n">on</span> <span class="n">BERT</span><span class="o">.</span><span class="err">’</span><span class="p">)</span>
<span class="n">roberta</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="err">‘</span><span class="n">mnli</span><span class="err">’</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="further-discussion">
  Further Discussion
  <a class="heading-link" href="#further-discussion">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p><strong>Transformers are extremely memory intensive</strong>. Hence, there is quite a high probability that we will run out of memory or above the runtime limit while training larger models or for longer epochs.</p>
<p>Hence, in my next blog, I’ll be discussing and implementing some <strong>promising well-known, and impactful out-of-the-box strategies to speed up transformer with optimization strategies to reduce the training time</strong>.</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      <footer class="footer">
  <section class="container">
    ©
    
      2021 -
    
    2022
     Aastha Singh 
    ·
    Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
  </section>
</footer>

    </main>

    
      
      <script src="/js/coder.min.03b17769f4f91ae35667e1f2a1ca8c16f50562576cf90ff32b3179926914daa5.js" integrity="sha256-A7F3afT5GuNWZ&#43;HyocqMFvUFYlds&#43;Q/zKzF5kmkU2qU="></script>
    

    

    

    

    

    

    

    

    
  </body>

</html>
